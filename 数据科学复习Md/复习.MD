
# 第一章
## 数据
### 数据的概念
在一定背景下有意义的对于现实世界中的事物定性或定量的记录。
各种符号（如字符、数字等）的组合、语音、图形、图像、动画、视频、多媒体和富媒体.

### 数据的结构化维度
|类型|含义|本质|举例|
|--|--|--|--|
|结构化数据|直接可以用传统关系数据库存储和管理|先有结构|后有数据|关系型数据库中的数据|
|非结构化数据|无法用关系数据库存储和管理的数据|没有（或难于发现）统一结构|语音，图像文件|
|半结构化数据|经过一定的转换可以用传统关系数据库存储和管理的数据|先有数据，后有结构	HTML,XML文件等|

## 大数据
### 大数据的定义
首先，大数据依旧是数据，或数据相关的过程，其次，大数据的规模并非一定要达到某一确切的数值，关键在于，是否超过了实际情况下的数据存储能力和数据计算能力。

### 大数据的内涵与特征
|学科	|参照物	|理解方式|
|-- |-- |-- |
|计算机科学	|现有的计算能力和存储能力	|当数据量、数据的复杂程度、数据处理的任务要求等超出了传统数据存储与计算能力时|
|统计学	|总体的规模	|当能够收集足够的全部（总体中的绝大部分）且计算能力足够大，可以不用抽样，直接在总体上进行统计分析|
|机器学习	|智能的实现方式	|当训练集足够大，计算能力足够强，只需要通过对已有实例进行简单查询就可以达到“智能计算”的效果|

### 大数据的特征
#### 4V
增长速度快；数据处理要求速度快

相对于计算与存储能力，数据量大

数据价值和数据量之间不存在线性关系

结构化、半结构化、非结构化数据（90%）

#### 5v
大量性  
高速性  
多样性  
真实性  
价值性

#### 5R模型
通过以数据管理的角度，认识从大数据中获取有用信息的过程得到。模型包括大数据的相关特性(Relevant)，实时特性(Real-time)，真实特性(Realistic)，可靠特性(Reliable)，以及投资回报特征(Return on investment, ROI)。

#### 4P医疗模型
4P医疗模型基于医疗大数据，包含了预测性(Predictive)，预防性(Preventive)，个体化(Personalized)，参与性(Participatory)。该医疗模型基于大数据，对疾病做出预测，并基于个人数据对病人做出个性化的服务，同时，诊疗过程中的数据将再次被记录到数据库中，从而为病人提供基于大数据的健康建议。

### 大数据时代的新理念
第一范式：实验科学范式

第二范式：理论科学范式：以模型和归纳为特征

第三范式：计算科学范式：以模拟和仿真为特征

第四范式：数据密集型科学发现范式：从大数据中查找和挖掘所需的信息和知识，无需直接面对所研究的物理对象。

## 数据科学
### 什么是数据科学
数据科学（Data Science）―― 是指以数据为中心的科学

数据科学的内涵：  
用数据的方法研究科学  
生物信息学、天体信息学、地球科学等  

用科学的方法研究数据  
统计学、机器学习、数据挖掘、数据库  

# 第二章
## 数据预处理概述

<table width="845.50" border="0" cellpadding="0" cellspacing="0" style='width:422.75pt;border-collapse:collapse;table-layout:fixed;'>
   <col width="278.70" style='mso-width-source:userset;mso-width-alt:6794;'/>
   <col width="95.90" style='mso-width-source:userset;mso-width-alt:2338;'/>
   <col width="375" style='mso-width-source:userset;mso-width-alt:9142;'/>
   <col width="95.90" style='mso-width-source:userset;mso-width-alt:2338;'/>
   <tr height="112" style='height:56.00pt;mso-height-source:userset;mso-height-alt:1120;'>
    <td class="xl65" height="348" width="278.70" rowspan="6" style='height:174.00pt;width:139.35pt;border-right:none;border-bottom:none;' x:str>数据科学导论</td>
    <td class="xl65" width="95.90" rowspan="2" style='width:47.95pt;border-right:none;border-bottom:none;' x:str>质量要求</td>
    <td class="xl65" width="375" style='width:187.50pt;' x:str>不一致性数据</td>
    <td width="95.90" style='width:47.95pt;' x:str>数据审计</td>
   </tr>
   <tr height="112" style='height:56.00pt;'>
    <td class="xl66" x:str>错误/虚假<font class="font0"><br/></font><font class="font0">无效数据</font><font class="font0"><br/></font><font class="font0">缺失数据</font><font class="font0"><br/></font><font class="font0">重复数据</font></td>
    <td x:str>数据清洗</td>
   </tr>
   <tr height="40" style='height:20.00pt;mso-height-source:userset;mso-height-alt:400;'>
    <td class="xl65" rowspan="4" style='border-right:none;border-bottom:none;' x:str>计算要求</td>
    <td class="xl65" x:str>数据格式/大小不符合处理要求</td>
    <td class="xl67" x:str>数据变换<font class="font0"><br/><br/><br/></font></td>
   </tr>
   <tr height="28" style='height:14.00pt;'>
    <td class="xl65" x:str>数据需要合并处理</td>
    <td x:str>数据集成</td>
   </tr>
   <tr height="28" style='height:14.00pt;'>
    <td class="xl65" x:str>数据缺少必要的标注信息</td>
    <td x:str>数据标注</td>
   </tr>
   <tr height="28" style='height:14.00pt;'>
    <td class="xl65" x:str>无序数据</td>
    <td x:str>数据排序</td>
   </tr>
   <![if supportMisalignedColumns]>
    <tr width="0" style='display:none;'>
     <td width="279" style='width:139;'></td>
     <td width="96" style='width:48;'></td>
     <td width="375" style='width:188;'></td>
     <td width="96" style='width:48;'></td>
    </tr>
   <![endif]>
  </table>

### 脏数据问题
不完整：缺少数据值；缺乏某些重要属性；仅包含汇总数据；例如：occupation=“   ”  

有噪声：包含错误或者孤立点。例如：Salary = -10： 

不一致
在编码或者命名上存在差异
等级：1、2、3； A、B、C；甲、乙、丙
重复记录间的不一致性
Age=“20” Birthday=“03/07/1997”

### 数据预处理概述
数据预处理将是构建数据仓库或者进行数据挖掘的工作中占工作量最大的一个步骤

### 数据质量
数据正确性（Correctness）：数据是否真实地记录了客观现象

数据完整性（Integrity）：数据是否被损坏或者未被授权
有些有价值的属性，如商品销售的数据中顾客的信息，并非总是可以得到的

一致性（Consistency）：数据内容之间是否存在自相矛盾的现象。


相关性 （Relevance）
特别是在工业界，对于数据质量的相关性要求是非常高的。在统计学和实验科学领域，强调精心设计实验来收集与特定假设相关的数据。许多数据质量问题与特定的应用和领域有关。

扩展属性
形式化程度（Formalization）：是否用规范化的表达方式进行表示，便于计算机理解；
时效性(Timeliness)：是否反映客观现实的最新状态；
精确性(Accuracy)：精度是否满足需求；
自描述性(Self-Description)：数据是否带有自描述信息，用于评价数据质量的高低；

### 属性
是一个数据字段，表示数据

标称属性：标称属性的值是一些符号或事物的名称。

二元属性：二元属性是一种标称属性，只有两个状态：0 或 1

序数属性：序数属性是一种属性，其可能的值之间具有有意义的序或秩评定（ranking），但是相继值之间的差是未知的。比如：对一个餐厅评价“优”，“良”，“差”

数据属性：是定量的可度量的量，用整数或实数表示。

离散属性：具有有限个或无限个可数个数，可以用或不用整数表示。

连续属性：如果属性不是离散的，则它是连续的。

### 统计学规律
第一数字定律(First-Digit Law)
描述的是自然数“1”到“9”的使用频率，公式为
$ P(d)=log_{10}{(d+1)}?log_{10}{d} $
其中，数字“1”的使用最多接近三分之一，“2”为17.6%，“3”为12.5%，依次递减，“9”的频率是4.6%

小概率原理:
   基本思想：一个事件如果发生的概率很小的话，那么它在一次试验中是几乎不可能发生的，但在多次重复试验中几乎是必然发生的，数学上称之小概率原理。在统计学中，把小概率事件在一次实验中看成是实际不可能发生的事件，一般认为等于或小于0.05或0.01的概率为小概率。

### 语言学规律
#### 频率特征
各字母在英语文章中出现的频率。。。
#### 连接特征
       包括语言学中的后连接（如字母“q”后总是“u”）、前连接（如字母“x”的前面总是字母“i”，字母“e”很少与“o”和“a”连接）以及间断连接（如在“e”和“e”之间，“r”的出现频率最高） 

#### 重复特征
      两个字符以上的字符串重复出现的现象，叫做语言的重复特征。例如在英文中字符串“th”、“tion”和“tious”的重复率很高。

### 数据连续性理论的主要研究内容
将原始数据分解成多个碎片数据，包括数据元的识别、抽取、转换和加载；
碎片数据的传播、演化和跟踪；
碎片数据的关联（难点）：将每个碎片数据与其它相关碎片数据、历史版本数据、相关主体及其它数据集进行关联；
碎片数据的分析、集成和利用。 

### 数据预处理的主要任务
1）描述性数据归总
2）数据清洗
填写空缺的值，平滑噪声数据，识别、删除孤立点，解决不一致性
3）数据集成
集成多个数据库、数据立方体或文件
4）数据变换和标准化
 规范化和聚集，提高涉及距离度量的挖掘算法  
    的准确性和有效性
5）数据归约
得到数据集的压缩表示，它小得多，但可以得到相同或相近的结果
6）数据离散化
数据归约的一部分，通过概念分层和数据的离散化来规约数据，对数字型数据特别重要
7）特征的选择与降维

## 描述性数据汇总

 动机：为了更好地理解数据，获得数据的总体印像、识别数据的典型特征、凸显噪声或离群点
度量数据的中心趋势：
       均值、中位数、众数、中列数
度量数据的离散程度：
       四分位数、四分位数极差、方差  
### 分布式度量
将函数用于n个聚集值得到的结果和将函数用于所有数据得到的结果一样  Count( )
Sum( )
Min( )
Max( )

### 代数度量
通过在一个或多个分布式度量上应用一个代数函数而得到
平均值函数Avg() 
         Avg() = Sum()/Count()

### 整体度量
整体度量(holistic)：必须对整个数据集计算的度量
median( )
mode( )
rank( )

### 中心趋势度量
算术平均值：
$ x=\frac{1}{n}\sum_{i=1}^{n}{x_i} $

加权算术平均
$x=\frac{\sum_{i=1}^{n}{w_ix_i}}{\sum_{i=1}^{n}{w_i}} $

截断均值：去掉高、低极端值得到的均值
   例: 计算平均工资时，可以截掉上下各2％的值后计算均值，以抵消少数极端值的影响
中位数：有序集的中间值或者中间两个值平均
众数：众数（Mode  模）集合中出现频率最高的值

### 离散程度度量
方差和标准差

正态分布函数曲线 (μ, σ)
从 μCσ 到 μ+σ：包含了大概 68% 的数据
从 μC2σ到 μ+2σ： 包含了大概 95% 的数据
从 μC3σ 到 μ+3σ：包含了大概 99.7% 的数据

极差（range）：数据集的最大值和最小值之差

百分位数(percentile)：
     第k个百分位数是具有如下性质的值x：k%的数据项位于或低于x，中位数就是第50个百分位数

四分位数：Q1 (25th percentile), Q3 (75th percentile)

中间四分位数极差(IQR)： IQR = Q3 C Q1 
孤立点：通常我们认为：挑出落在至少高于第三个四分位数或低于第一个四分位数 1.5×IQR 处的值

盒图

直方图

分位数图:设xi 是递增排序的数据，则每个xi都有相对应的 fi ，指出大约有 f i ％的数据小于等于 xi

散布图

局部回归曲线:曲线为散布图添加一条平滑的曲线，以便更好地观察两个变量间的依赖模式；





## 缺失值处理
### 数据缺失的原因分析
数据采集过程可能会造成数据缺失

数据通过网络等渠道进行传输时可能出现数据丢失或出错； 

在数据整合过程中可能引入缺失值

### 缺失值的处理策略
第一类方法是删除带有缺失值的样本或特征； 
第二类方法是采用某种方法对缺失值进行填补:均值填补
随机填补
基于模型的填补

#### 均值填补法
  均值填补法首先计算该特征中非缺失值的平均值或众数，然后使用平均值或众数来代替缺失值
对于连续型特征，通常使用平均值进行填补；
对于离散型特征，则使用众数进行填补.?

#### 随机填补法
随机填补是在均值填补的基础上加上随机项，通过增加缺失值的随机性来改善缺失值分布过于集中的缺陷； 
随机填补方法包括贝叶斯Bootstrap方法和近似贝叶斯Bootstrap方法。

贝叶斯Bootstrap方法:假设数据集 有?n?个样本，某特征?f?存在?k?个非缺 失值和 ?(n?k)?个缺失值，使用贝叶斯 Bootstrap方法进行缺失值填补共有两步：
第一步：从均匀分布?U(0,?1)中随机抽取?k?1个随机数，并进行升序序记为{0,?a_1,a_2,?…,a_k?1,?1}；第二步：对?(n?k)?个缺失值，分从非缺失值?{f_1, f_2,… , f_k} 中以概率?a_1,a_2?a_1,?…,1?a_k?采样一个值进行填补.

近似贝叶斯Bootstrap方法首先从?k?个非缺失值f_1, f_2,… , f_k中有放回地抽取k个值建立一个新的大小为k的集合F. 然后对于 (n??k)个缺失值，分别从?F 中随机抽取一个值进行填补

基于模型的填补方法:
基于模型的方法将缺失特征?f?作为预测目标. 将数据集中其他特征或其子集作为输入特征，通过特征f的非缺失值构造训练集，训练分类或回归模型. 然后使用构建的模型来预测特征f的缺失值

哑变量方法:例如学生信息数据集中，将"性别"特征的缺失值作为一个特殊的取值"unknown"

EM填补方法:
EM算法可以用来进行缺失值填补，此时缺失特征被当做隐含变量.

## 数据转换与编码
### 数据的标准化
**0-1标准化（Min-Max标准化）：**
> 该方法的核心即是对数据系列作线性变换，使得处理过后数据均落在［0，1］区间内
$x=\frac{x_i-min}{max-min}$
若希望标准化后的数据以0为中心落在［－1,1］区间内
$x_i^?=\frac{x_i?\frac{max+min}{2}}{\frac{max?min}{2}}$
（1）首先找到样本数据Y的最小值Min及最大值Max
（2）计算系数为：k=（b-a）/(Max-Min)
（3）得到归一化到[a,b]区间的数据：norY=a+k(Y-Min)

**小数定标标准化**
通过移动数据的小数点位置来进行标准化，

**Z-Score  标准化**
> 处理后的数据具有固定均值和标准差
假设原取值集合为?{f_1,?f_2,?f_n}，则?f_i??经过?Z?Score 标准化后：
$F=\frac{f_i-\mu}{\sigma}$
$\mu$平均值
$\sigma=\sqrt{\frac{1}{n}\sum_{i=1}^{n}{(f_i-\mu)^2}}$标准差
反映每个数据点距离平均值点的标准差距离
均值将落在0附近，而每一个数据点离零点的距离可解释为其远离均值的标准差距离
适用于数据系列中最大值或最小值未知、数据分布非常离散的情况
当数据中存在离群点时，为了降低离群值的影响，将标准差替换成平均绝对差：
$s=\frac{1}{n}\sum_{i-1}^{n}{|f_i-\mu|}$

**Logistic 标准化**
> 利用Sigmoid函数的特性，将原始数据系列转化为［0，1］之间的数。
$S(x)=\frac{1}{1+e^{-x}}$
适用于数据系列分布相对比较集中地分布于零点两侧

|方法	|优点	|缺点	|适用范围|
|---|---|---|---|
|Z-Score|	转化为标准正态分布无序数据的最大、最小值|	需要记录原数据均值方差|数据中最大最小值未知且数据系列分布离散|
|0-1|	对数据作线性变换,保留数据的原始关系|	若原数据最大最小值发生变化需重新定义|	需要保留原始数据间的关系，且最大最小值已经确定|
|小数定标	|简单实用、易于还原标准化后的数据|	若原数据最大绝对值发生变化需重新定义|	数据系列分布比较离散，数据遍布多个数量级|
|Logistic	|简单，通过单一映射函数对数据进行标准化|	对分布零散且远离零点的数据处理效果不佳|	数据系列分布比较集中，且均分布于零点两侧|

### 数据的编码
**数据转换**
> 处理这些非数字型特征就成为一个关键的问题

**数字编码**
> 会引入错误的序；
会引入错误距离

**One-hot编码:**
> 独热编码,又称一位有效编码
其**方法**是使用N位状态寄存器来对N个状态进行编码，每个状态都有他独立的寄存器位，并且在任意时候，其中只有一位有效（为1），其余全为0。可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征，并且这些特征互斥，每次只有一个激活，因此，数据会变成稀疏的。
**美国（1,0,0）、英国（0,1,0）、法国（0,0,1）**
**优点**:不会给名义型特征的取值人为地引入次序关系；
经过One-Hot编码之后，不同的原始特征取值之间拥有相同的距离；
在线性回归模型中，对名义型特征进行One-Hot编码的效果通常比数字编码的效果要好. One-Hot编码对包含离散型特征的分类模型的效果有很好的提升.
**缺点**:
特征维度会显著增多；
    假设存在10个包含100个取值的离散型特征，经过One-Hot编码后的特征数量将变成1000个.
它会增加特征之间的相关性

**哑变量编码(one-hot改变):**
> 对于一个包含K个取值的离散型特征，将其转换成K-1个二元特征，这种编码方法称为哑变量编码 
**美国（1,0）、英国（0,1）、法国（0,0）**
## 数据离散化
将连续性特征转换成为离散型特征的过程称为**特征离散化**（data discretization）
将连续性特征的取值范围划分为若干区间段（bin）,用区间段代替落在该区间段的特征取值；
区间段之间的分割点称为切分点（cut point）；
分割 出来的区间段的个数称为元数（arity）
### 离散化步骤
特征排序：对连续型特征的取值进行升序或者降序排列，减少离散化的运算开销；
切分点选择：根据给定评价准则，合理选择切分点，常用的评价准则是基于信息增益或者基于统计量；
区间段分割或者合并：基于选择好的切分点，对现有区间段进行分割或者合并，得到新的区间段
在生成的新的区间段上重复以上步骤，直到满足终止条件

### 离散化概述

#### 自顶向下的离散化方法

**等距离散化**
> 根据连续特的取值，将其均匀地划分成 k 个区间，每个区间的宽度相等，区间宽度 ω
$\omega=\frac{f_{max}-f_{min}}{k}$
对数据质量要求高
对离群值敏感

**等频离散化**
> 当特征取值分布不均匀，等距离散化后，区间段中的样本可能出现严重不均衡，为解决这个问题，不再限定宽度一致，而是使得离散化后每个区间内样本量均衡。  
保证了每个区间段有相同的样本数
取值相近的样本会被划分到不同区间

**聚类离散化**
> 样本能落到相同的区间段内
**步骤**
对需要离散化的连续性特征，采用聚类算法（K-means, EM），把样本依据该特征划分成相应的簇或者类；
在聚类的结果上，基于特定的策略，决定是否对簇进行进一步的分裂或合并，利用自顶向下的策略针对每一个簇继续运行聚类算法，细分成为更小的子簇，或者利用自底向上的策略对相邻的簇进行合并；
在最终确定划分簇后，确定切分点及区间个数。
**举例**
假设操场上有20个学生随机地站在那不动，我们想把他们分成5组，用K-Means算法该怎么分呢？
第1步，我们在这20个同学里面抽出5个同学作为小组长；
第2步，剩下的15个同学，每个同学都量量他（她）自己和第1步中选定的小组长的距离，把自己归到离他（她）最近的那个小组长那一组，经过第2步我们就初步的把20个同学分成类5组了（每一组的同学个数不一定是4个）；
第3步，在第2步中得到的5个组，我们再按一定办法给每个组指定一个新的小组长；
第4步，在第3步中没有被选中为小组长的剩下的15个同学重新计算自己与新的小组长的距离，把自己归到离自己最近的那个新的小组长那组；
第5步，重复上面的工作，一直到我们对分组的效果满意了或者重复的次数达到我们设置的上限了。

**信息增益离散化**
- 该方法源自于决策树模型，在建立决策树时，遍历每一个特征，选择熵最小（信息增益最大）的特征作为正式分裂节点
- **步骤**
    - 对连续型特征进行排序
    - 把特征的每一个取值作为候选分裂节点（切分点），计算出相应的熵，选择熵最小的取值作为正式的切分点，将原来的区间一分为二；
    - 递归处理第二步中得到的两个新区间段，直到每个区间段内特征的类别一样为止；
    - 合并相邻的，类的熵值为0且特征类别相同的区段，重新计算新区间段类的熵值；
    - 重复第四步到满足终止条件（决策树的深度或叶子数）   




#### 自底向上的离散化方法
**卡方离散化**
- 将特征的取值看作单独的区间，然后逐一递归进行区间合并。
- 卡方检验
  - 卡方检验属于非参数检验范畴，是一种比较两个总体之间是否存在显著性差异的方法
  - $x^2=\sum_{i}^{k}{\frac{(A_i-E_i)^2}{E_i}}$
  $$A_i为落入区间段的样本个数，也就是观察频数；$$
  $$E_i为对应的期望频数，在 n 较大的情况下，$$
  $$χ^2统计量近似服从自由度为(k-1)的 χ^2 分布$$
  - ChiMerge方法  
  ChiMerge通过卡方检验，判断相邻区间是否需要合并，也就是说，区间内特征取值的类别要独立于区间。
    - 步骤：
      - 将连续型特征的每一个取值看作是一个单独的区间段，并进行排序；
      - 针对每对相邻的区间段，计算卡方统计量。卡方值最小或者低于设定阈值的相邻区间段合并
      - $$χ^2=\sum_{i=1}^{2}{\sum_{j=1}^{C}{\frac{(A_{ij}?E_{ij})^2}{E_ij}}}$$
      - 其中$$A_ij$$为第i 区间段内类别为j 的样本个数，k?为比较的区间个数，C为类别个数，且有$$E_{ij}=\sum_{j=1}^{C}{A_{ij}?\frac{\sum_{i=1}^{k}{A_{ij}}}{n}}$$
      - 对于新的区间段，递归进行第1,2步，只到满足终止条件
    - 分析
      -  每次迭代的过程只能合并两个区间，如果数据量大，则算法的开销会比较大
      -  需要设定显著性水平，然后计算卡方统计量，通过阈值的设定来控制区间数的多少。
  - 类别属性依赖最大化（CAIM）离散化
    -  采取自顶向下的策略。通过选择切分点??p?，把特征的取值空间划分为?f≤p?和 f>p?两个子区间段, 用来衡量切分点选择优劣的度量方法是类别属性的相互依赖程度。
    -  假设某个连续型特征有?n?个取值，C?个类别. 假设我们把特征划分为?k?个子区间段，子区间段集合记为：
           D={[d_0,d_1],(d_1,d_2],…,(d_k?1,d_k]}
其中$d_0$和$d_k$分别为特征的最小值和最大值, $n_{i?}$表示属于类别?i?的样本个数, $n_?j$表示落在区间段($d_{j?1}$,$d_j$]的样本个数，$n_ij$表示在区间内($d_{j?1},d_j$]的且属于类别?i?的样本个数. 我们可以得到一个由类别特征和离散化特征取值所构成的二维表
![caim二维表](.\image\复习\CAIM.png)
计算一个评价准则来评价当前离散化的好坏：$CAIM=\frac{1}{N}\sum_{j=1}^{K}{\frac{M_j^2}{n_{\cdot j}}}$
$M_j$=max{$n_{1j},??n_{2j},??,?n_{Cj}$}? CAIM 的取值为区间(0, 1]
    - CAIM的值越大，说明类和离散区间的相互依赖程度越大，也就说明了离散化效果越好
    - 步骤：
      - 对进行离散化的特征进行升序排列，确定取值区间的最   
    小值 d_0和最大值d_k，初始化划分策略D=[d_0,d_k]；
      -  把区间内的每个值当作候选切分点，计算把区间二分后的
    CAIM值，并选取CAIM值最高的点作为切分点，并更新D
      -  对于D中的每个区间段，重复第二步的计算，直到满足终
    止条件.
    - CAIM只关注区间内拥有最多样本的类别与特征之间的关系，忽略同一区间内其他类别的信息， CAIM最终生成的离散化区间个数往往与样本的类别个数接近。

![离散化总结](.\image/复习/离散化总结.png)


## 特征选择与降维
**目的**：特征选择能剔除不相关(irrelevant)或冗余(redundant )的特征，从而达到减少特征个数，简化模型，提高模型精确度，减少运行时间的目的
![特征选择流程图](.\image/复习/特征选择流程图.png)
  - 子集产生：按照一定的搜索策略产生候选特征子集；可以从空集、随机产生一个特征子集、整个特征子集开始，分别采用向前、向后、或者双向搜索；
  - 子集评估：通过某个
评价函数评估特征子集
的优劣，与历史最优值
进行比较；
  - 停止条件：
基于子集的方式：达到指定的特征数量或者迭代次数
基于子集评估的方式：增加或者减少特征不能再提高评价值或者达到设定的阈值。
  - 子集验证： 验证最终所选的特征子集的有效性
**常用的线性特征提取方法：**
  - 主成分分析（PCA）
  - 线性判别分析（LDA）
**常用的非线性特征提取方法：**
  - 多维尺度变换（MDS）
  - 局部线性嵌入（LLE）
**主成分分析**
构造原始特征的一系列线性组合形成低维的特征，以去除数据的相关性，并使降维后的数据最大程度地保持原始高维数据的方差信息

协方差$cov(x,y)=\frac{\sum_{i=1}^{n}{(x_i?\overline{x})(y_i?\overline{y} )}}{n?1}$

特征向量，特征值分解

**PCA模型**
就是找出数据里最主要的方面，用数据里最主要的方面来代替原始数据。具体的，假如我们的数据集是n维的，共有m个数据 
 。我们希望将这m个数据的维度从n维降到n'维，希望这m个n'维的数据集尽可能的代表原始数据集。我们知道数据从n维降到n'维肯定会有损失，但是我们希望损失尽可能的小。

PCA最优化的方差等于原数据集 X?的协方差矩阵的特征值之和

拉格朗日乘子法

低维空间的特征都写成高维特征的线性加权形式，为理解低维数据提供了便利
主成分分析没有需要调参的超参数，具有全局最优解，不存在局部最优解的问题
计算效率较高，其时间复杂度为O(d^3)，空间复杂度为?(d^2 )
当数据中存在线性结构且方差较大时，通常可以选择主成分分析作为首选降维方法
方差最大化有时并不是我们感兴趣的优化目标

**线性判别分析（LDA）**
使用数据的类别信息，将高维的样本线性投影到低维空间中，使得低维表示最有利于对数据进行分类

LDA的目标是利用样本的类别标签信息，找到一个利于数据分类的线性低维表示
这个目标可以从两个角度来量化
第一个角度是使得降维后相同类样本尽可能近。使用类内离散度(within-class scatter)（类内样本的方差）度量
第二个角度是使得降维后不同类样本尽可能远。使用类间离散度(between-class scatter)（不同类样本的均值的方差）度量

**收缩判别分析**
$?S_w $
进行调整
**LDA与PCA区别**
基本思想不同
  - PCA选择样本投影具有最大方差的方向，最大化保留了数据的内部信息
  - LDA则考虑标签信息，使得投影后不同类之间的样本距离最大化以及同类样本距离最小化
学习模式不同
  - PCA属于无监督式学习，适用范围更广，但并不能保证数据降维后数据易于分析
  - LDA属于有监督学习，同时具有分类和降维的能力
**多维尺度变换MDS**
目标是找到数据的低维表示，使得降维前后样本之间的相似度信息尽量得以保留

只利用样本间的距离信息，找到每一个样本的特征表示，且在该特征表示下样本的距离与原始的距离尽量接近

 度量多维尺度变换(metric MDS)，即对于任意样本 ? 、样本 ? 和样本 ? ，距离矩阵 ? 满足以下条件：
$d_{ik}+d_{jk}≥d_{ij}$(三角形两边之和大于第三边)
 对于度量多维尺度变换，如果我们选择欧式距离来计算样本间的距离，则得到的低维表示将和主成分分析的结果是一致的
 当不等式的条件不满足时，MDS不再以保持样本距离为目标，而是尽量保持样本间距离的次序，此时的方法称为非度量多维尺度变换(non-metric MDS)

**局部线性嵌入（LLE）**
将数据降到低维空间中，但是保留数据局部的线性关系
每一个样本点可以写成其 ? 个近邻点的线性组合，从高维嵌入（embedding）到低维时尽量保持该局部线性关系
第一步  选择近邻样本
第二步  局部线性重构
第三步  寻找低维表示
**优点**
- LLE只包含两个需要调参的超参数：近邻样本数量 ? 和正则化系数 ?
- LLE算法得到的是全局最优解，不会陷入局部最优值。这些优点使得LLE在图像识别、图像分类和数据可视化领域得到广泛的应用
**缺点**
- 由于需要使用K近邻算法寻找近邻样本，当样本量不足时，样本与其近邻样本距离可能较远，导致降维效果变差；
- LLE对噪音和异常值比较敏感，因而当数据中存在异常值时效果可能受到影响；
- 没有显式的映射对应关系（例如PCA、LDA: Y=XW）
- LLE假设样本分布在一个单独的光滑流形，对于分类问题该假设是不成立的。
**核主成分分析KPCA**
利用核方法来弥补传统主成分分析只能进行线性降维的缺陷

基本思想：非线性映射 ? 将原始数据集 ? 映射到更高维的空间中。在更高维空间中的样本 ?(?) 中利用 PCA 进行降维 
主要应用：信号去噪和人脸识别

**等度量映射Isometric mapping**
用测地距离（曲线距离）作为空间中两点距离，原来是用欧氏距离，从而将位于某维流形上的数据映射到一个欧氏空间上
优点：Isomap 能较好保留数据的全局结构
缺点：对噪音比较敏感，而且计算复杂度也比较高 

**扩散映射**
基于动态系统的非线性降维方法
基本思想：通过构建一个扩散图，使用图中的扩散距离来度量样本之间的相似度。与Isomap 使用最短路径来度量样本相似性不同的是，扩散映射本质上考虑了样本间的所有路径
优点：扩散映射避免了矩阵分解等操作，具有较好的抗噪能力


t分布随机近邻嵌入  
自编码器(autoencoders)、
塞曼映射(Sammon mapping)
拉普拉斯特征映射( Laplacian eigenmaps)

**数据脱敏**
单向性
无残留
易于实现
**数据规约**
维归约
- 主成分分析（Principal Component Analysis，PCA）
- 奇异值分解（Singular Value Decomposition，SVD）
- 离散小波转换（Discrete Wavelet Transform，DWT）

值规约
- 参数模型(如简单线性回归模型和对数线性模型等)
- 非参数模型(如抽样、聚类、直方图等)

**数据标注**
语法标注
语义标注

数据预处理工作往往有一定代价的
导致数据损失，甚至可能对数据产生曲解

必要性的讨论可以归结为两个问题：
一是我们是否接受“数据的复杂性”；
二是我们的计算能力是否足以解决数据中的复杂性问题


# 第三章(分类与回归)
## 概述
### 分类概述
一种典型的有监督学习问题

分类目标属性y是离散的，回归目标属性y是连续的

分类一般分为以下两个阶段：
- 分类器训练：即通过训练样本的特征和标签来建立分类模型
- 分类预测：利用分类模型对没有分类标签的数据进行分类

数据集划分为训练集和测试集
- 训练集训练分类器
- 测试集用于评估分类性能



## 决策树
**步骤**
- 首先对数据进行处理，利用归纳算法生成可读的规则和决策树；
- 然后使用决策对新数据进行分析。
**特点**
- 推理过程容易理解，决策推理过程可以表示成 If  Then形式；
- 推理过程完全依赖于属性变量的取值特点；
- 可能忽略目标变量没有贡献的属性变量，这也为判断属性变量的重要性，减少变量的数目提供参考。
### CLS
从一棵空决策树开始，选择某一属性（分类属性）作为测试属性，该测试属性对应决策树中的决策结点，根据该属性的值的不同将训练样本分成相应的子集：
如果该子集为空，或该子集中的样本属于同一个类，则该子集为叶结点；
否则该子集对应于决策树的内部结点，需要选择一个新的分类属性对该子集进行划分，直到所有的子集都为空或者属于同一类。（其实就是分裂到节点里面都是同一类就是叶子）
### ID3 算法
不纯度 (impurity)：表示落在当前节点的样本类别分布的均衡程度，节点分裂后，节点不纯度应该更低，选择特征及对应分割点，使得分裂前后的不纯度(impurity)下降最大。

节点 ? 的信息熵：
$Entropy(t)=-\sum_{c=1}^{c}{p(c|t)\log_2{p(c|t)}}$
p(c|t)=c|t
t为节点内部点数
c为某同一类的的点的点数

节点分裂前后的信息熵的下降值，称为信息增益
$infoGain=Entropy(t_0)-\sum_{k=1}^{k}{\frac{n_k}{n}Entropy(t_k)}$
$n_k是子节点t_k的样本数量，n是父节点t_0的样本数量$
挑选信息熵增益最大的特征进行分裂

Gini 指数是 20 世纪初意大利学者 Gini 用来判断社会收入分配公平程度的指标，它能够反映社会中各个收入水平的人群数量分布的均衡程度 ：
节点 ? 的Gini指数：
$Gini(t)=1-\sum_{c=1}^{c}{[p(c|t)]^2}$
当样本均匀分布在每一个类中， Gini指数为 1?1/?, 说明不纯度大
当样本都分布在同一个类中， Gini指数为0，说明不纯度小


误分率:
$Error(t)=1-max(p(1|t),p(2|t),...,p(C|t))$

ID3 算法存在什么问题:
倾向于分裂成很多的小节点（节点的样本数较小），容易造成过拟合 

从ID3到C4.5




## K近邻
当对测试样本进行分类时，找到训练集中与该样本集最相似的?个样本，根据?个样本的标签确定测试样本的标签

算法流程
确定?的大小和距离计算方法
从训练样本中得到?个与测试样本最近的样本
根据?个最相似的训练样本的类别，通过投票的方式来确定测试样本的类别

明可夫斯基距离(通用公式)：$d(x_1,x_2)=(\sum_{i=1}^{n}{|x_{1i}-x_{2i}|^p})^{(\frac{1}{p})}$
曼哈顿距离（x与y之差之和）
欧氏距离（勾股定理）
切比雪夫（p=+无穷）（max(| x2-x1| , |y2-y1| )）
余弦距离
杰卡德距离
1-（两个集合的交集除以集合的并集个数）

简单实用，易于实现
对异常数据不敏感，具有较好的抗噪性
K近邻算法的计算效率不高
当训练集较小的时候，K近邻算法易导致过度拟合




## 回归
面对一个具体问题，给定样本集合D={(x_1,?y_1),?…,?(x_n,?y_n)}，目标是找到一条直线  $y=ω_0+ω_1x$  使得所有样本点尽可能落在它的附近。

模型诊断
$
R^2=1-\frac{SS_{res}}{SS_{tot}}
$
$
SS_{tot}=\sum(y_i-(y真实值平均值)^2)
$
$
SS_{res}=\sum(y_i-(y模型预估值)^2)
$

处理过拟合问题常用的方法：
- 减少特征数量 ?主要方法有：人工的挑选重要的特征，去除不重要的特征，采用模型选择算法；去除特征的同时，也去除了这部分特征所提供的信息；
- 正则化（Regularization） 保留所有特征，但是减少参数?ω?的值。**(不知道在讲啥)**



## SVM
支持向量机(Support Vector Machine，SVM) 

分类的思想是在这个二维平面中能找到一个决策边界（在二维例子中这个决策边界是一条直线），在决策边界下方的都是蓝点 在决策边界上方的都是红点

对同一个问题存在多个分类函数时，哪一个函数更好呢？显然必须要先找一个指标来量化“好”的程度    ―― 分类间隔

分割线的特点是，找到红蓝点中离这根直线最近的点，同时尽可能使得这些点离这根直线尽可能的远。 总的来说就是这根直线离蓝色的样本尽可能远，离红色样本也尽可能远。

泛化能力：不只是注重模型对样本的拟合度，更注重的是模型对于新数据的适应能力如何？（也就是对未知的数据进行判断分类的能力）。

核函数Kernel的作用就是隐含着一个从低维空间到高维空间的映射，而这个映射可以把低维空间中线性不可分的两类点变成线性可分的，其目的与多项式特征中升维是一样的道理

高斯核函数RBF

分类问题中如何找决策边界
算法模型的泛化能力
SVM是如何最优化选择决策边界，提升泛化能力：最大化d
Hard Margin SVM的改进，具有容错能力的Soft Margin SVM
如何解决线性不可分的样本数据 添加特征多项式 升维
核函数如何简化升维后计算的复杂度
SVM在多分类，回归问题中的解决思路



## 朴素贝叶斯分类
朴素贝叶斯分类算法是根据训练集，推导出一个概率公式，然后计算在新客户的违约概率，根据客户违约概率的大小，确定将该客户归为违约类还是归为守信类

对于无法直接看出概率的
$p(Y│X)=P(X|Y)P(Y)/P(X)$

$P(?X,??Y?)=P(?Y?|?X?)P(?X?)$

$P(?X,?Y?)=P(Y,X)$

$P(?Y,??X?)=P(?X?|?Y?)P(?Y?)$

拉普拉斯平滑

当特征是连续变量时，通常有两种处理方法
第一种方法是将连续的特征离散化
第二种方法：采用高斯模型，假定连续型特征服从正态分布，使用正态分布对条件参数进行求解。

高斯分布?:
$P(X_i=x│Y=c)=\frac{1}{\sqrt{2π}σ}e^{?\frac{(x?μ)^2}{2σ^2}}$

多项式模型
伯努利模型

朴素贝叶斯算法的参数估计就是估计先验参数P(Y=k)、条件参数P(X=Xi|Y=k)
条件参数P(X=Xi|Y=k)，通常会参考以下三种常见的模型求解
多项式模型
伯努利模型
高斯模型

优点：
- 朴素贝叶斯算法发源于古典数学理论，有着坚实的数学基础、良好的分类效率；
- 算法逻辑简单、易于理解，只需要使用贝叶斯公式转化即可，易于实现；
- 对小规模数据表现良好，能处理多分类任务，适合增量式训练；
- 特征之间相互独立，只会涉及二维存储，分类算法过程开销小；
- 对缺失数据不敏感，常用于文本分类。
  
缺点：
- 朴素贝叶斯算法假设特征之间是相互独立的，但是这种假设在现实中往往是难以成立的，特征之间总会存在相关性，在属性个数多且属性之间的相关性比较大时，分类效果不理想；
- 分类过程中需要知道先验概率和条件参数，而先验概率和条件参数的求解过程取决于我们选择的模型，当选择不合适的模型时，求得的先验概率和条件参数的值也会影响分类的结果。







## 关联规则挖掘
置信度：p（尿布|啤酒）
支持度：尿布啤酒同时次数/全部购物数据总数



需要自主判断该关联规则是否有趣，即计算的支持度和置信度是否达到预期，这个预期被称为最小支持度阈值和最小置信度阈值，当某个关联规则同时满足这两个预选设好的阈值，便把它称为强关联规则。

步骤
- 先找出所有满足最小支持度的商品集合；
- 通过这些集合生成关联规则；
- 最后利用置信度筛选出强关联规则。?

事务：每一条购物信息可以被定义为一个事务；

项：购物信息中的一项物品被定义为项；

项集：包含零个项或多个项的集合被称为项集，含有k个项的项集称为k项集；

关联规则就可以表示为形如“A=>B” 的蕴涵式，其中A，B均为非空项集，且A∩B= ?。

支持度大于预定义的最小支持度阈值的项集称为频繁项集。

从数据集中挖掘强关联规则的过程分为两步：
- 第一步：需要找出满足最小支持度阈值的项集，即频繁项集；
- 第二步：根据最小置信度阈值，从频繁项集中生成强关联规则。

**Apriori**性质：
- 如果一个项集A是频繁项集，那么它的非空子集B也是频繁项集。
- 或者：如果一个项集不是频繁项集，那它的超集也不是频繁项集。
- 优点：
Apriori算法利用Apriori性质对候选项集生成过程进行了优化，采用了剪枝，避免了很多无效计算，简化了频繁项集生成过程，是有效的关联规则挖掘算法。
- 缺点：
Apriori算法每一次迭代都需要遍历数据集，当数据集过大无法一次性载入内存时，遍历的效率会变得非常低；
Apriori算法仍然会产生大量无用的候选多项集，例如有1000个频繁1项集，则需要生成C21000个候选2项集，这说明还需要改进关联规则挖掘算法

**FP-Growth**算法
FP树是一种输入数据的压缩表示，通过逐个读取事务，把每个事务映射到一条路径来构造，由于不同的事务可能具有若干个相同的项，因此路径可能会重叠，重叠越多，压缩效果越好；如果FP树足够小能存放到内存中，就不必要重复扫描硬盘来提取频繁项集

过程
- 第一步根据原始数据集构造频繁模式树（FP树）;
- 第二步基于频繁模式树挖掘频繁项集。

条件模式库：是指从原始数据集的频繁树中投影出的子数据集，它与特定后缀相关联




## 集成算法

集成多个模型的能力，得到比单一模型更佳的效果

基模型集成策略：
- 多数投票方法 (majority vote)
- 平均 (averaging)
- 加权平均 (weighted averaging)：如AdaBoost

**Bagging**
- 对样本或特征随机取样，学习产生多个独立的模型，然后平均所有模型的预测值
- 主要减小方差
- 典型代表随机森林

Bagging是“Bootstrap aggregating”的缩写
该方法的核心包括自助抽样(Bootstrap)和平均(Aggregating) 

对于分类问题，每个分类器返回类预测，采用多数投票的方法
对于回归问题，采用平均值的方法

特别适合用来提高那些方差大但偏差小的基模型(决策树，神经网络等)的预测性能 
单个模型不稳定：对训练数据轻微的改变就能够造成分类器性能很明显的变化
使用Bagging可以综合投票结果，从而提升稳定性以及准确率
便于并行化。多个抽样数据的获取及基模型的训练互相没有关联，可以方便地进行并行计算 

决策树的局限性
局部最优问题
- 决策树由于使用贪婪算法的思想，即在每次分割时选择当前情况下带来信息增益最高的特征进行分割，容易陷入局部最优

分类边界问题
- 单棵的决策树在确定分类边界时，由于决策特征只涉及单个特征的逻辑判断，导致决策边界是平行于坐标轴的直线，这就限制了决策树对分类边界的表达能力，导致模型的准确性较差

**随机森林**的算法原理
随机森林使用并汇总多棵决策树进行预测，所以即使每棵树的决策能力很弱，由它们组合起来形成的随机森林的决策能力也会较强

分类间隔 (margin)：正确分类某样本的决策树的比例减去错误分类样本决策树的比例 

袋外误差 (Out-Of-Bag Error, OOB)：随机森林对袋外样本的预测错误率 

优点：
- 能够处理很高维度的数据，并且不用做特征选择
- 对特征之间存在的多重共线性不敏感，并且能够在一定程度上处理缺失数据和不均衡数据 
- 在训练完后能够给出哪些特征比较重要
- 容易做成并行化方法

缺点：
- 处理噪音较大的小样本和低维数据集的问题上会过度拟合
- 相对于决策树，预测速度较慢
- 相对于决策树，模型可解释性较差




**Boosting**
- 串行训练多个模型，后面的模型是基于前面模型的训练结果（误差）
- 代表是AdaBoost

Boosting假设基模型是弱分类器，即分类性能比随机猜测稍好的分类器
目标是通过一定的策略提升弱分类器效果，得到一个综合的强分类器

基本思想:
从训练数据集中训练得到一个弱分类器，下一个弱分类器基于前一个弱分类器的表现，重点关注前一个弱分类分错的样本，尽量将前一个弱分类器分错的样本分对
不同于Bagging并行的方式，Boosting是以串行方式训练获得强分类器
在Boosting方法中，权重赋给每一个训练样本，迭代地学习 ? 个分类器
学习得到分类器h_t之后，更新权重，使得其后的分类器h_t+1更关注h_t误分类的训练样本
最终的分类器 ?集成每个个体分类器的预测结果
需要预先设置弱分类器误差的上界

**AdaBoost**
AdaBoost的核心思想是利用同一训练样本的不同加权版本，训练一组弱分类器，然后把这些弱分类器以加权的形式集成起来，形成一个最终的强分类器：

在每一步迭代过程中，会给训练集中的样本赋予一个权重(w_1,w_2,…,w_n)；
样本的初始权重都一样，设置为 1/?；
在每一步迭代过程中，被当前弱分类器分错的样本的权重会相应得到提高，被当前弱分类器分对的样本的权重则会相应降低；
弱分类器的权重则根据当前分类器的加权错误率来确定。

优点
- 防止过拟合
- 不仅能减少方差还能减小偏差
- 除了弱分类器的数目T以外，没有额外的参数需要调参
- AdaBoost算法提供的是一种框架，弱分类器可以选择决策树或者其他分类算法

缺点：
- 由于集成了多个弱分类器，模型的可解释性降低
- 当弱分类器太复杂或者效果太差时，容易导致过拟合
- 对于异常值比较敏感



集成方法概述
Bagging和随机森林
Boosting和AdaBoost

# 第四章
## 聚类模型概述
聚类模型的本质：将数据集中相似的样本进行分组的过程
每个组称为一个簇(cluster)每个簇的样本对应一个潜在的类别 
样本是没有类别标签的，因此聚类是一种典型的无监督学习任务 ，这些簇满足以下两个条件
相同簇的样本之间距离较近
不同簇的样本之间距离较远

**常用的聚类模型**
- 划分聚类
- 层次聚类
- 基于密度


## 划分聚类

### K-means模型
K-means 的目标是要将数据点划分为 K 个簇，找到每个簇中心?c_k? ，并且最小化所有样本点到所属簇中心的距离平方和

- 选择K个点作为初始质心
-  Repeat：
将每个点指派到最近的质心形成K个簇
重新计算每个簇的质心
-  直到质心不发生变化

K-means算法的优点
算法实现简单、直观
K-means算法的缺点
事先指定 ? 值，聚类结果依赖于? 个初始质心的选择
容易陷入局部最优，不易处理非簇状数据
聚类结果容易受离群值影响
时间复杂度: ?(????)
? 为特征维度、? 为样本数量、 ? 为簇数量、 ? 为迭代次数
每一步迭代中，每个样本需要与 ? 个簇中心进行距离计算


**K-medoids聚类**（K中心点聚类）
为了改进 K-means 算法容易受离群值影响的问题，K-medoids 算法选取每个簇中到簇内其他点的距离和最小的样本作为簇的质心，是原始数据集中的某个点

**二分K-means**
每次从所有簇中选择具有最大 ? 值的簇，然后划分为两个簇，重复该过程，直至簇集合中含有 ? 个簇

**K-medians聚类**：使用样本每个维度的中位数作为簇的质心

**K-means++聚类** ：在初始化质心的过程中，选择相互距离尽可能远的样本作为初始质心

**基于粗糙集的K-means聚类**：在基于粗糙集的K-means算法中，一个样本可以被划分给多个簇


## 层次聚类
层次聚类(hierarchical clustering)在不同层级上对样本进行聚类，逐步形成树状的结构 

根据层次分解是以自底向上（合并）还是自顶向下（分裂）方式，层次聚类方法可以分为
- 聚合式聚类(agglomerative clustering)
  - 在开始时把每个样本都当成一簇，然后在每一次迭代中将最相似的(距离最近)两个簇进行合并，直到把所有簇合并为包含所有样本的一簇
  - 簇间距离的计算
    - 单连接
也称为最近邻距离，即簇 ? 和簇 ? 之间的距离定义为两簇之间最近的成员之间的距离
    - 完整连接
也称为最远邻距离，即簇 ? 和簇 ? 之间的距离定义为两簇之间最远的成员之间的距离
    - 平均连接
表示两簇间所有成员对的平均距离
- 分拆式聚类(divisive clustering)：分拆式聚类将所有样本集合看作一簇，以自上而下的方式，递归地将现有的簇分拆为两个子簇
  - 二分K-means聚类
选择半径最大的簇，对该簇进行K-means聚类分为两个子簇
重复此过程直到达到想要的簇个数
  - 最小生成树法
将每个样本看作一个图节点，将样本间距离看作节点间边的权重，根据此图建立最小生成树
从权重最大处将该簇分拆为两簇，然后重复此过程直到达到想要的簇个数。实际上，该方法得到的聚类结果和单连接的聚合聚类得到的结果一致


层次聚类一次性地得到了整个聚类的过程，想要分多少个簇都可以直接根据树图来得到结果，改变簇的数目不需要再次计算数据点的归属类别;
单连接和完全连接代表了簇间距离度量的两个极端，它们对离群点或噪声数据过分敏感;
平均连接是一种折中方法，它可以克服离群点敏感性问题
层次聚类的缺点是计算量大，而且错分在层次聚类中是不可修正的，一旦某个样本被分到某个聚类中，则该样本永远停留在该聚类中


## 基于密度的聚类
此类算法假设聚类结构由样本分布的紧密程度确定，以数据集在空间分布上的稠密程度为依据进行聚类，即只要一个区域中的样本密度大于某个阈值，就把它划入与之相近的簇中。

DBSCAN是一种著名的密度聚类算法，它基于一组邻域参数(?，Minpts)刻画样本分布的紧密程度，寻找数据集中的高密度以及低密度区域来完成聚类。可以将该算法归纳为三个步骤：设定邻域参数，数据点分类，聚类。
- 第一步：设定邻域参数(?，Minpts)
以各个数据点为中心，以?为半径，计算其?-邻域内的数据点密度。其中Minpts代表一个簇中最少的数据点个数，高于这个值则为高密度区域，否则为低密度区域。用户可以根据数据集的实际密度的不同设置这两个参数。
- 第二步：数据点分类
基于邻域参数，在DNSCAN算法中，所有的数据点可以分为以下三个类别：
核心点:如果某个点的邻域内的数据点数目高于阈值minpts，则将这个点视为核心点。
边界点:边界点是位于核心点邻域之内的，但是其自身的邻域内数据点数小于minpts的点，起到将高密度区域与低密度区域分割开的作用。
噪声点:既不是核心点也不是边界点的其他数据点，他们组成低密度区域。
- 第二步：数据点分类
进行密度聚类之前还需要了解
- 第三步：聚类
下面三个概念：
密度直达：两个同属于一个邻域的数据点是密度直达关系
密度可达：o在p的邻域内，从p到o是密度直达，而q对象的邻域内不包括p，但是包括o，这样p->o->q，称p到q是密度可达的。密度相连：q和p是密度可达的, q和t也是密度可达的，则p和t是密度相连的。
  - DBSCAN的目标为找到密度相连数据点的最大集合，此集合作为最终的一簇。首先核心点各自成簇，采用密度相连的概念逐步对簇进行合并，打上标记。
最终核心点密集的区域会被低密度噪声点包围，噪声点不单独成簇。所以采用DBSCAN进行聚类，最终的结果有一些数据点是没有标记的，这些就是噪声点。

优点：
无需事先设定簇的个数，算法根据数据自身找出各簇
适于稠密的非凸数据集，可以发现任意形状的簇
可以在聚类时发现噪音点、对数据集中的异常点不敏感
对样本输入顺序不敏感
缺点：
因为是基于密度分析，如果客观存在的两个簇没有明显的可分间隔，则很有可能被合并为同一个簇
同时DBSCAN聚类的参数调节较为复杂，参数设置对结果影响较大
DBSCAN对高维的数据处理效果不好

### pagerank算法
不只考虑词项，还考虑指向该网页的链接情况
被越多优质的网页所指的网页，它是优质的概率就越大

















